{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAAP Science Use Case Estimates\n",
    "\n",
    "The document estimates the size of data that's being queried/used while running various DPS algorithms by the MAAP User Working Group.\n",
    "\n",
    "Based on the UWG notebooks ([here](https://repo.ops.maap-project.org/icesat2_boreal/icesat2_boreal/-/tree/master/dps)), we gathered the queries that were made and used them to get an estimate of the data quantities and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "%pip install geopandas\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import requests\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries made by the algorithm were either to the MAAP cmr or NASA cmr or to a AWS bucket.\n",
    "\n",
    "Here, we're setting up the cmr hosts.\n",
    "\n",
    "We're also reading the Boreal and Copernicus DEM indices that have already been created and are in the files `dem30m_tiles.geojson` and `boreal_grid_albers90k_gpkg.gpkg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up cmr hosts\n",
    "NASA_HOST = \"cmr.earthdata.nasa.gov\"\n",
    "MAAP_HOST = \"cmr.maap-project.org\"\n",
    "\n",
    "# Reading the indices for boreal and copernicus\n",
    "dem = gpd.read_file(\"dem30m_tiles.geojson\")\n",
    "boreal = gpd.read_file(\"boreal_grid_albers90k_gpkg.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now creating some functions that'll calculate the total size of the queried files from CMR.\n",
    "Each of these functions have docstrings that explain what they do, what the input and output are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_from_results(response, format, seen):\n",
    "    \"\"\"\n",
    "    Based on the response from the post call, format and the seen list,\n",
    "    gives the total size and num granules from the response\n",
    "\n",
    "    :param response: response from the post call to the cmr\n",
    "    :param format: format of the data (json or umm_json)\n",
    "    :param seen: list of seen granules (to avoid duplicates)\n",
    "\n",
    "    :return: total size in bytes and number of granules\n",
    "    \"\"\"\n",
    "\n",
    "    # HLS requires umm_json because the default json doesn't have the size of the granule\n",
    "    if format == \"umm_json\":\n",
    "        results = response[\"items\"]\n",
    "        # Only keep the granules whose `GranuleUR` is not in the seen list\n",
    "        unique = [\n",
    "            result for result in results if result[\"umm\"][\"GranuleUR\"] not in seen\n",
    "        ]\n",
    "        total_size = sum(\n",
    "            [\n",
    "                float(\n",
    "                    granule[\"umm\"][\"DataGranule\"][\"ArchiveAndDistributionInformation\"][\n",
    "                        0\n",
    "                    ][\"SizeInBytes\"]\n",
    "                )\n",
    "                for granule in unique\n",
    "            ]\n",
    "        )\n",
    "        num_granules = len(unique)\n",
    "        # Add the granuleUR to the seen list\n",
    "        seen.extend([result[\"umm\"][\"GranuleUR\"] for result in unique])\n",
    "    else:\n",
    "        # For the non-HLS case, the size is in the json response\n",
    "        # We repeat the same process as above, but with the json response\n",
    "        results = response[\"feed\"][\"entry\"]\n",
    "        unique = [\n",
    "            result for result in results if result[\"producer_granule_id\"] not in seen\n",
    "        ]\n",
    "\n",
    "        total_size = sum([float(result[\"granule_size\"]) for result in unique])\n",
    "        num_granules = len(unique)\n",
    "\n",
    "        seen.extend([result[\"producer_granule_id\"] for result in unique])\n",
    "\n",
    "    return total_size, num_granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_estimates(query_dict, host=\"maap\", format=\"json\", seen=[]):\n",
    "    \"\"\"\n",
    "    Based on the query params (query_dict) to cmr (host: maap or nasa), format and seen list (for de-duplication),\n",
    "    gives size estimates\n",
    "\n",
    "    :param query_dict: dict of query params that the cmr accepts\n",
    "    :param host: cmr host (maap or nasa)\n",
    "    :param format: format of the data (json or umm_json)\n",
    "    :param seen: list of seen granuleids (to avoid duplicates)\n",
    "\n",
    "    :return: dict of total_size (in bytes) and num_granules\n",
    "    \"\"\"\n",
    "    # Setting the host and url based on the inputs\n",
    "    host = MAAP_HOST if host == \"maap\" else NASA_HOST\n",
    "    base_url = f\"https://{host}/search/granules.{format}\"\n",
    "\n",
    "    headers = {}\n",
    "\n",
    "    # Initialize the size and count\n",
    "    total_size = 0\n",
    "    num_granules = 0\n",
    "\n",
    "    # The cmr results are paginated, so we need to loop through the pages until all the granules are accounted for\n",
    "    while True:\n",
    "        if num_granules and (num_granules % 1000 == 0):\n",
    "            print(f\"{num_granules} granules processed\")\n",
    "\n",
    "        # Making the post call to cmr\n",
    "        response = requests.post(base_url, data=query_dict, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            # Calculate size and granule count and add them up\n",
    "            size, num_grans = get_size_from_results(response.json(), format, seen)\n",
    "            total_size += size\n",
    "            num_granules += num_grans\n",
    "\n",
    "            # If there's more granules in the next page, update header and run again, else break\n",
    "            if search_after := response.headers.get(\"CMR-Search-After\"):\n",
    "                headers = {\"CMR-Search-After\": search_after}\n",
    "            else:\n",
    "                print(\"No more granules\")\n",
    "                break\n",
    "        # If the response is not 200, print the error and break\n",
    "        else:\n",
    "            print(\"Response status code:\", response.status_code)\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "    return {\"total_size\": total_size, \"count\": num_granules}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel 1A/1B SLC estimates\n",
    "\n",
    "| Collection | Hosted in | Datetime Range |\n",
    "|------------|-----------|----------------|\n",
    "| SENTINEL 1A SLC | NASA CMR | 2019 |\n",
    "| SENTINEL 1B SLC | NASA CMR | 2019 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 1A and 1B concept ids\n",
    "concept_ids = [\"C1214470488-ASF\", \"C1327985661-ASF\"]\n",
    "# Build the query params for Sentinel 1A and 1B SLC to send to cmr, with the given temporal range\n",
    "sentinel_dict = {\n",
    "    \"pageSize\": 2000,\n",
    "    \"temporal\": \"2019-01-01T00:00:00Z,2019-12-31T23:59:59Z\",\n",
    "}\n",
    "\n",
    "# Call the get_size_estimates function to get the size estimates\n",
    "# The data is hosted in the nasa cmr (thus, host=\"nasa\")\n",
    "sentinel_estimates = {\"total_size\": 0, \"count\": 0}\n",
    "\n",
    "# Looping through the concept ids and aggerating the size estimates\n",
    "for concept_id in concept_ids:\n",
    "    sentinel_dict[\"collection_concept_id\"] = concept_id\n",
    "    estimates = get_size_estimates(sentinel_dict, host=\"nasa\")\n",
    "    sentinel_estimates[\"total_size\"] += estimates[\"total_size\"]\n",
    "    sentinel_estimates[\"count\"] += estimates[\"count\"]\n",
    "\n",
    "print(sentinel_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATL estimates\n",
    "| Collection | Hosted in | Datetime Range | Bounding box |\n",
    "|------------|-----------|----------------|--------------|\n",
    "| ATL08 v5 | MAAP CMR | Apr 1st to Oct 30th, 2019-2021 | -180, 50, 180, 75 |\n",
    "| ATL03 v4 | MAAP CMR | 2018-10-14 to 2021-07-15 | -180, 50, 180, 75 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATL08 v5 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the query params for ATL08 v5 to send to cmr, with the given temporal range and bounding box\n",
    "atl08_dict = {\n",
    "    \"collection_concept_id\": \"C1201746153-NASA_MAAP\",\n",
    "    \"pageSize\": 2000,\n",
    "    \"temporal[]\": [\n",
    "        \"2019-04-01T00:00:00Z,2019-10-30T23:59:59Z\",\n",
    "        \"2020-04-01T00:00:00Z,2020-10-30T23:59:59Z\",\n",
    "        \"2021-04-01T00:00:00Z,2021-10-30T23:59:59Z\",\n",
    "    ],\n",
    "    \"bounding_box\": \"-180,50,180,75\",\n",
    "    \"provider\": \"NASA_MAAP\",\n",
    "}\n",
    "\n",
    "# Call the get_size_estimates function to get the size estimates\n",
    "# The data is hosted in the maap cmr (host is defaulted to \"maap\")\n",
    "atl08_estimates = get_size_estimates(atl08_dict)\n",
    "print(atl08_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATL03 v4 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the query params for ATL03 v4 to send to cmr, with the given temporal range and bounding box\n",
    "atl03_dict = {\n",
    "    \"collection_concept_id\": \"C1201300747-NASA_MAAP\",\n",
    "    \"pageSize\": 2000,\n",
    "    \"temporal\": \"2019-06-01T00:00:00Z,2019-09-30T23:59:59Z\",\n",
    "    \"bounding_box\": \"-180,50,180,75\",\n",
    "    \"provider\": \"NASA_MAAP\",\n",
    "}\n",
    "\n",
    "# Call the get_size_estimates function to get the size estimates\n",
    "# The data is hosted in the maap cmr (host is defaulted to \"maap\")\n",
    "atl03_estimates = get_size_estimates(atl03_dict)\n",
    "print(atl03_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copernicus DEM estimates\n",
    "\n",
    "For Copernicus DEM, the data files are stored in AWS S3. To get the specific files used, we do an intersection of the Boreal index with the DEM index and get all the files from the intersection. All the file size are then aggregated to calculate the total estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copernicus_s3_list():\n",
    "    \"\"\"\n",
    "    Gets all the s3 files from the dem index that intersect with the boreal index\n",
    "    \"\"\"\n",
    "    # Get the intersection of indices from boreal and copernicus dem\n",
    "    selection = dem[dem.intersects(boreal.to_crs(\"EPSG:4326\").unary_union)]\n",
    "    # Get all the s3 file urls from the intersection\n",
    "    return selection[\"s3\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_aws(list_of_urls):\n",
    "    \"\"\"\n",
    "    Gets the cumulative size of all the files from the list_of_urls (s3 urls)\n",
    "\n",
    "    :param: list_of_urls: list of s3 urls in the form s3://<bucket>/<path to file>\n",
    "\n",
    "    :return: total size in bytes and count of files\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    total_size = 0\n",
    "    count = 0\n",
    "    for url in tqdm(list_of_urls):\n",
    "        # Get bucket and key from the s3:// url\n",
    "        split_url = url.replace(\"s3://\", \"\").split(\"/\")\n",
    "        bucket, key = split_url[0], \"/\".join(split_url[1:])\n",
    "\n",
    "        # Make a head request to get the size of the file\n",
    "        response = s3.head_object(Bucket=bucket, Key=key)\n",
    "\n",
    "        # Add to size and increment count\n",
    "        total_size += response[\"ContentLength\"]\n",
    "        count += 1\n",
    "\n",
    "    return {\"total_size\": total_size, \"count\": count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls the get_size_aws function to get the size estimates for all the s3 urls\n",
    "copernicus_estimates = get_size_aws(\n",
    "    # Get the list of s3 urls from the dem intersection boreal indices\n",
    "    get_copernicus_s3_list()\n",
    ")\n",
    "print(copernicus_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HLS v2 estimates\n",
    "| Collection | Hosted in | Datetime Range | Bounding box |\n",
    "|------------|-----------|----------------|--------------|\n",
    "| HLS v2 | NASA CMR | years 2019-2021, June 1st - Sept 15th | Boreal Index |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reverse_polygon(polygon):\n",
    "    \"\"\"\n",
    "    CMR supports counter-clockwise polygons as query params, this function converts the clock-wise polygons that geopandas gives to counterclockwise polygons that cmr accepts\n",
    "\n",
    "    :param polygon: polygon in the form [x1, y1, x2, y2, ...] in clockwise order\n",
    "\n",
    "    :param: polygon in the form \"x1,y1,x2,y2,...\" in counterclockwise order (format that cmr accepts)\n",
    "    \"\"\"\n",
    "    reversed = []\n",
    "    # Read the polygon from end to start\n",
    "    for index in range(len(polygon) - 1, -1, -2):\n",
    "        # Add a pair of coordinates to the reversed list\n",
    "        reversed.append(polygon[index - 1])\n",
    "        reversed.append(polygon[index])\n",
    "    return \",\".join(reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boreal_polygons():\n",
    "    \"\"\"\n",
    "    Get all the polygons bounds from boreal index, used to make calls to cmr\n",
    "    \"\"\"\n",
    "    polygons = []\n",
    "    # Regex to find all the floating point numbers in a string\n",
    "    float_regex = \"[+-]?[0-9]*[.][0-9]+\"\n",
    "\n",
    "    for polygon in boreal.to_crs(\"EPSG:4326\")[\"geometry\"]:\n",
    "        # Finds all the floating point numbers (which are coordinates) in the polygon\n",
    "        long_lats = re.findall(float_regex, f\"{str(polygon)}\")\n",
    "        # Reverses them and adds to the polygons list\n",
    "        polygons.append(_reverse_polygon(long_lats))\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the polygons from the boreal index\n",
    "polygons = get_boreal_polygons()\n",
    "\n",
    "# Build query params for the cmr call for HLS v2\n",
    "hls_dict = {\"collection_concept_id\": \"C2021957295-LPCLOUD\", \"pageSize\": 2000}\n",
    "\n",
    "# Initialize an empty list to store the granuleids that are already accounted for (to avoid duplicates)\n",
    "hls_seen_granules = []\n",
    "\n",
    "# Initialize total size and number of granules\n",
    "total_size = 0\n",
    "num_granules = 0\n",
    "\n",
    "# Iterate through years 2019 - 2021\n",
    "for year in range(2019, 2022):\n",
    "    # For each of the polygon, we make a cmr call to get the granules, and add them to the total\n",
    "    # We use the hls_seen_granules list to avoid duplicates\n",
    "    for polygon in tqdm(polygons):\n",
    "        hls_dict[\"polygon\"] = polygon\n",
    "        hls_dict[\"temporal\"] = (f\"{year}-06-01T00:00:00Z,{year}-09-15T23:59:59Z\",)\n",
    "        result = get_size_estimates(\n",
    "            hls_dict, host=\"nasa\", format=\"umm_json\", seen=hls_seen_granules\n",
    "        )\n",
    "\n",
    "        total_size += result[\"total_size\"]\n",
    "        num_granules += result[\"count\"]\n",
    "\n",
    "hls_estimates = {\"total_size\": total_size, \"count\": num_granules}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all the estimates related to Boreal to get the total size estimates\n",
    "\n",
    "The Boreal calculations consist of all the previously mentioned datasets: ATL03, ATL08, Copernicus DEM and HLS. Here, we combine all the estimates together to get the cumulative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_boreal = [\n",
    "    atl03_estimates,\n",
    "    atl08_estimates,\n",
    "    copernicus_estimates,\n",
    "    hls_estimates,\n",
    "]\n",
    "combined_boreal_estimates = {\n",
    "    \"total_size\": sum([x[\"total_size\"] for x in combined_boreal]),\n",
    "    \"count\": sum([x[\"count\"] for x in combined_boreal]),\n",
    "}\n",
    "print(combined_boreal_estimates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
